---
title: "Clusterisation variables à expliquer"
params:
  carb: 1
---
```{r, include=FALSE}
# Imporation des données
set.seed(123456)
rm(list=ls())
setwd("/home/rogerbernat/Documents/Projet_statistique_2024")
donnees <- readRDS("base_PC_Var_X_Var_Y_ENSAI_Respi_FINALE.RData", "rb")[,c(2,3,4,5)]
donnes_na <- na.omit(donnees)
scaled_data <- scale(donnees)
scaled_data_na <- na.omit(scaled_data)

# install.packages(c("FactoMineR", "umap", "RColorBrewer", "reshape", "car"))
```
## ACP
On effectue une ACP :

```{r}
library(FactoMineR)
donnees.PCA <- PCA(scaled_data_na, graph=FALSE)
plot(donnees.PCA, choix="var")
```

Observation des corrélations :

* Les éternuements et la toux pendant l'engraissement sont fortement corrélés entre eux.
* Les variables liées au post-sevrage sont également fortement corrélées entre elles, mais indépendantes des variables d'engraissement.


```{r}
plot(donnees.PCA$ind$coord)
```

En gardant les deux premiers axes, on obtient une variance expliquée supérieure à 60 %. On observe une distribution triangulaire des individus. On s'intéresse à la contribution des individus : 
```{r}
plot(donnees.PCA$ind$contrib)

top_contrib.1 <- order(donnees.PCA$ind$contrib[, 1], decreasing = TRUE)[1:7]
top_contrib.2 <- order(donnees.PCA$ind$contrib[, 2], decreasing = TRUE)[1:3]
top_contrib <- unique(c(top_contrib.1, top_contrib.2))
top_contrib

```

Observons que lors de la création des axes, huit individus ont principalement contribué à la création. Nous les excluons ensuite et effectuons à nouveau une ACP pour vérifier si cette répartition en forme de triangle est due à leur présence.
```{r}
donnees.PCA2 <- PCA(scaled_data_na, ind.sup = top_contrib)
plot(donnees.PCA2, choix="var")

```

On peut faire les mêmes observations que dans l'ACP précédente concernant la variance expliquée et la corrélation des variables. Examinons maintenant la distribution des individus selon ces axes :
```{r}
plot(rbind(donnees.PCA2$ind$coord, donnees.PCA2$ind.sup$coord))
```

On fait la même remarque que tout à l'heure. Regardons les contributions :
```{r}
plot(donnees.PCA2$ind$contrib)
```

Cependant, si on s'intéresse à la classification des individus sur les deux premiers axes, on arrive à distinguer que les individus semblent se diviser en 3 groupes :
```{r}
class.hierarchique1 <- hclust(dist(rbind(donnees.PCA2$ind$coord[,c(1, 2)], donnees.PCA2$ind.sup$coord[,c(1, 2)]), method = "euclidean"), method="ward.D2")
clusters1 <- cutree(class.hierarchique1, k = 3)
plot(class.hierarchique1)
```
```{r}
pca_data <- as.data.frame(rbind(donnees.PCA2$ind$coord[,c(1, 2)], donnees.PCA2$ind.sup$coord[,c(1, 2)]))
pca_data$Cluster <- as.factor(clusters1)

plot(pca_data$Dim.1, pca_data$Dim.2, col = pca_data$Cluster, pch = 19, 
     xlab = "PCA Dimension 1", ylab = "PCA Dimension 2", 
     main = "PCA Plot Colored by Clusters")
```

```{r, warning=FALSE}
library(reshape)
meltData <- melt(scaled_data_na)
library(RColorBrewer)
colors <- brewer.pal(3, "Set2")

boxplot(value ~ rep(clusters1, each = 4) * X2, data = meltData,
        horizontal =  TRUE,           
        outline = FALSE,  
        col = colors,  
        frame = FALSE,
        at = c(1,2,3,5,6,7,9,10,11,13,14,15),
        main = "Répartition des variables à expliquée par cluster acp",
        xlab = "Valeur",
        ylab = "",
        las = 2,
        cex.axis = 0.72,
        names=c("", "PS_TX", " ", " ", "PS_ETER", " ", " ", "ENG_TX", " ", " ", "ENG_ETER", " " )
        )

```

On observe le même phénomène que pour la première ACP, un petit group d'individus contribue fortement à la création des axes tandis que la contribution du reste des indivus reste relativement faible. De plus, les classes ne sont pas interprétables.

## UMAP 

Ensuite on se propose d'utiliser une autre méthode de réduction des dimensions afin de comparer les résultats, c'est le cas de umap, qui est plus adaptée pour des variables avec des comportements non linéaires:

```{r}
library(umap)
donnees.umap <- umap(scaled_data_na, n_components = 2)
plot(donnees.umap$layout)
```

Cette fois-ci, on n'observe plus la répartition triangulaire. Passons à l'étape de classification hiérarchique :
```{r}
class.hierarchique <- hclust(dist(donnees.umap$layout, method = "euclidean"), method="ward.D2")
clusters <- cutree(class.hierarchique, k = 3)

plot(class.hierarchique)

```

Au vu du dendogramme, on décide de faire une classification en trois clusters qui se distribuent dans la plan ainsi :
```{r}
clusters <- cutree(class.hierarchique, k = 3)
plot(donnees.umap$layout, col = colors[clusters], pch = 19,
     main = "UMAP Layout with 3 Clusters",
     xlab = "UMAP 1", ylab = "UMAP 2")
```

Passons à l'interprétation des clusters:

```{r}
clusters <- as.factor(clusters)

boxplot(value ~ rep(clusters, 4) * X2, data = meltData,
        horizontal =  TRUE,           
        outline = FALSE,  
        col = colors,  
        frame = FALSE,
        at = c(1,2,3,5,6,7,9,10,11,13,14,15),
        main = "Répartition des variables à expliquer normalisées par cluster umap",
        xlab = "Valeur",
        ylab = "",
        las = 2,
        cex.axis = 0.72,
        names=c("", "PS_TX", " ", " ", "PS_ETER", " ", " ", "ENG_TX", " ", " ", "ENG_ETER", " " )
        )
```

* Cluster orange : Contient majoritairement les individus malades en post-sevrage.
* Cluster vert : Contient les individus malades en engraissement, bien que cet effet soit moins marqué.
* Cluster bleu : Représente les individus en bonne santé.

Regardons plus en détail ses observations : 
```{r,  warning=FALSE}
library(car)
par(mfrow = c(2, 2))
for (i in 1:4) {

    densityPlot(donnes_na[, i] ~ clusters,
            xlab = colnames(donnes_na)[i],
            col = colors,
            legend=FALSE,
            normalize = TRUE)
}
df <- donnes_na[clusters != 3, ]

for (i in 1:4) {

    densityPlot(df[, i] ~ clusters[clusters != 3 ],
            xlab = colnames(donnes_na)[i],
            col = colors,
            xlim=c(-2, 45),
            legend=FALSE,
            normalize = TRUE
            )
}
```

On observe que les individus du groupe bleu se concentrent autour de zéro pour toutes les variables, ce qui suggère qu'ils sont généralement en bonne santé en post-sevrage et en engraissement. En ce qui concerne les classes orange et verte, on remarque qu'en post-sevrage, les individus de la classe verte se concentrent davantage autour de zéro que ceux de la classe orange. En revanche, en engraissement, les individus de la classe orange présentent davantage de toux et d'éternuements, tandis que la classe verte montre moins de symptômes. Ainsi, il semble que les élevages de la catégorie verte tendent à avoir des porcs en moins bonne santé en engraissement, tandis que ceux de la catégorie orange semblent rencontrer davantage de problèmes en post-sevrage.

Cette analyse rejoint les conclusions obtenues précédemment avec le barplot.


Passons aux tests statistiqes pour confirmer les observations.
```{r}
donnes_na$y <- clusters

for(variable in colnames(donnes_na)[1:4]){
        for(i in 1:2){
                for(j in (i+1):3){
                test <- wilcox.test(donnes_na[donnes_na$y == i,variable], donnes_na[donnes_na$y == j, variable], exact = FALSE)
                cat(sprintf("Wilcox Test pour la variable : %s\n", variable))
                cat(sprintf("Pour les clusters %d et %d\n", i, j))
                cat(sprintf("p-valeur est %f\n", test$p.value))
                cat("-------------------------\n")
                }
        }
}

for(variable in colnames(donnes_na)[1:4]){
  for(i in 1:2){
    for(j in (i+1):3){
      
      # Perform the Kolmogorov-Smirnov test
      test <- ks.test(donnes_na[donnes_na$y == i, variable], donnes_na[donnes_na$y == j, variable])
      
      # Print formatted result
      cat(sprintf("Kolmogorov-Smirnov Test pour la variable : %s\n", variable))
      cat(sprintf("Pour les clusters %d et %d\n", i, j))
      cat(sprintf("p-valeur est %f\n", test$p.value))
      cat("-------------------------\n") 
                }
        }
}

```
En effectuant les test statistiques, on constate que les répartitions des clusters par rapport aux valeurs dans les quatre variables à expliquée sont bien différentes ainsi que les moyennes. 
L'analyse visuel réalisé est donc validé.

RESTE A FAIRE TABLEAU DES REPARTITIONS, min-max etc