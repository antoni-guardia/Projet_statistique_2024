---
title: "Clusterisation variables à expliquée"
params:
  carb: 1
---
```{r, include=FALSE}
# Imporation des données
set.seed(123456)
rm(list=ls())
setwd("/home/rogerbernat/Documents/Projet_statistique_2024")
donnees <- readRDS("base_PC_Var_X_Var_Y_ENSAI_Respi_FINALE.RData", "rb")[,c(2,3,4,5)]
donnes_na <- na.omit(donnees)
scaled_data <- scale(donnees)
scaled_data_na <- na.omit(scaled_data)

# install.packages(c("FactoMineR", "umap", "RColorBrewer", "reshape", "car"))
```
## ACP
On effectue une ACP :

```{r}
library(FactoMineR)
donnees.PCA <- PCA(scaled_data_na, graph=FALSE)
plot(donnees.PCA, choix="var")
```

Observation des corrélations :

* Les éternuements et la toux pendant l'engraissement sont fortement corrélés entre eux.
* Les variables liées au post-sevrage sont également fortement corrélées, mais indépendantes des variables d'engraissement.


```{r}
plot(donnees.PCA$ind$coord)
```

En gardant les deux premiers axes, on obtient une variance expliquée supérieure à 60 %. On observe une distribution triangulaire des individus. On s'intèresse à la contribution des individus : 
```{r}
plot(donnees.PCA$ind$contrib)

top_contrib.1 <- order(donnees.PCA$ind$contrib[, 1], decreasing = TRUE)[1:7]
top_contrib.2 <- order(donnees.PCA$ind$contrib[, 2], decreasing = TRUE)[1:3]
top_contrib <- unique(c(top_contrib.1, top_contrib.2))
top_contrib

```

Observons que lors de la création des axes, huit individus ont principalement contribué à la création. Nous les excluons ensuite et effectuons à nouveau une ACP pour vérifier si cette répartition en forme de triangle est due à leur présence.
```{r}
donnees.PCA2 <- PCA(scaled_data_na, ind.sup = top_contrib)
plot(donnees.PCA2, choix="var")

```

On peut faire les mêmes observations que dans l'ACP précédente concernant la variance expliquée et la corrélation des variables. Examinons maintenant la distribution des individus selon ces axes :
```{r}
plot(rbind(donnees.PCA2$ind$coord, donnees.PCA2$ind.sup$coord))
```

On fait la même remarque que tout à l'heure. Regardons les contributions :
```{r}
plot(donnees.PCA2$ind$contrib)
```

Cependant, si on s'intèresse à la classification des indivus sur les deux premiers axes, on arrive a distinguer que les individus s'emblent se diviser en 3 groupes :
```{r}
class.hierarchique1 <- hclust(dist(rbind(donnees.PCA2$ind$coord[,c(1, 2)], donnees.PCA2$ind.sup$coord[,c(1, 2)]), method = "euclidean"), method="ward.D2")
clusters1 <- cutree(class.hierarchique1, k = 3)
plot(class.hierarchique1)
```
```{r}
pca_data <- as.data.frame(rbind(donnees.PCA2$ind$coord[,c(1, 2)], donnees.PCA2$ind.sup$coord[,c(1, 2)]))
pca_data$Cluster <- as.factor(clusters1)

plot(pca_data$Dim.1, pca_data$Dim.2, col = pca_data$Cluster, pch = 19, 
     xlab = "PCA Dimension 1", ylab = "PCA Dimension 2", 
     main = "PCA Plot Colored by Clusters")
```

```{r, warning=FALSE}
library(reshape)
meltData <- melt(scaled_data_na)
library(RColorBrewer)
colors <- brewer.pal(3, "Set2")

boxplot(value ~ rep(clusters1, each = 4) * X2, data = meltData,
        horizontal =  TRUE,           
        outline = FALSE,  
        col = colors,  
        frame = FALSE,
        at = c(1,2,3,5,6,7,9,10,11,13,14,15),
        main = "Répartition des variables à expliquée par cluster acp",
        xlab = "Valeur",
        ylab = "",
        las = 2,
        cex.axis = 0.72,
        names=c("", "PS_TX", " ", " ", "PS_ETER", " ", " ", "ENG_TX", " ", " ", "ENG_ETER", " " )
        )

```

On observe le meme phenomene que pour la premiere ACP, un petit group d'individus contribue fortement à la création des axes tandis que l'apportation du reste des indivus reste relativement faible. De plus, les classes ne sont pas interpretables.

## UMAP 

Ensuite on se propose d'utiliser une autre méthode de réduction des dimensions afin de comparer les resultats, c'est le cas de umap, qui est plus adaptée pour des variables avec des comportements non lineaires:

```{r}
library(umap)
donnees.umap <- umap(scaled_data_na, n_components = 2)
plot(donnees.umap$layout)
```

Cette fois-ci, on n'observe plus la repartition rectangulaire. Passons à l'étape de classification hierarchique :
```{r}
class.hierarchique <- hclust(dist(donnees.umap$layout, method = "euclidean"), method="ward.D2")
clusters <- cutree(class.hierarchique, k = 3)

plot(class.hierarchique)

```

Au vu du dendogramme, on decide de garder aue trois clusters qui se distribuent dans la plan ainsi :
```{r}
clusters <- cutree(class.hierarchique, k = 3)
plot(donnees.umap$layout, col = colors[clusters], pch = 19,
     main = "UMAP Layout with 3 Clusters",
     xlab = "UMAP 1", ylab = "UMAP 2")
```

Passons à l'interpretation des clusters:

```{r}
clusters <- as.factor(clusters)

boxplot(value ~ rep(clusters, 4) * X2, data = meltData,
        horizontal =  TRUE,           
        outline = FALSE,  
        col = colors,  
        frame = FALSE,
        at = c(1,2,3,5,6,7,9,10,11,13,14,15),
        main = "Répartition des variables à expliquée normalisées par cluster umap",
        xlab = "Valeur",
        ylab = "",
        las = 2,
        cex.axis = 0.72,
        names=c("", "PS_TX", " ", " ", "PS_ETER", " ", " ", "ENG_TX", " ", " ", "ENG_ETER", " " )
        )
```

* Cluster orange : Contient majoritairement les individus malades en post-sevrage.
* Cluster vert : Contient les individus malades en engraissement, bien que cet effet soit moins marqué.
* Cluster bleu : Représente les individus en bonne santé.

Regardons plus en detail ses observations : 
```{r,  warning=FALSE}
library(car)
par(mfrow = c(2, 2))
for (i in 1:4) {

    densityPlot(donnes_na[, i] ~ clusters,
            xlab = colnames(donnes_na)[i],
            col = colors,
            legend=FALSE,
            normalize = TRUE)
}
df <- donnes_na[clusters != 3, ]

for (i in 1:4) {

    densityPlot(df[, i] ~ clusters[clusters != 3 ],
            xlab = colnames(donnes_na)[i],
            col = colors,
            xlim=c(-2, 45),
            legend=FALSE,
            normalize = TRUE
            )
}
```

On observe que les individus du groupe bleu se concentrent autour de zéro pour toutes les variables, ce qui suggère qu'ils sont généralement en bonne santé en post-sevrage et en engraissement. En ce qui concerne les classes orange et verte, on remarque qu'en post-sevrage, les individus de la classe verte se concentrent davantage autour de zéro que ceux de la classe orange. En revanche, en engraissement, les individus de la classe orange présentent davantage de toux et d'éternuements, tandis que la classe verte montre moins de symptômes. Ainsi, il semble que les élevages de la catégorie verte tendent à avoir des porcs en moins bonne santé en engraissement, tandis que ceux de la catégorie orange semblent rencontrer davantage de problèmes en post-sevrage.

Cette analyse rejoint les conclusions obtenues précédemment avec le barplot.



```{r, include=FALSE}
# Stockage de la variable dans le data frame : 

# donnees$y <- NA

# lignes_non_manquantes <- !apply(is.na(scaled_data), 1, any)

# donnees$y[lignes_non_manquantes] <- clusters
# donnees$y
# saveRDS(donnees, file="nouvelles_donnees.Rdata")
# donnees2 <- readRDS("nouvelles_donnees.Rdata")
# donnees2$y
# summary(donnees)
```



