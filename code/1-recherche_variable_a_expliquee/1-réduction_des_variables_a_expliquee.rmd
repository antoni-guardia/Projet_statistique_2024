---
title: "Clusterisation variables à expliquer"
params:
  carb: 1
---
```{r, include=FALSE}
# Imporation des données
set.seed(123456)
rm(list=ls())
setwd("/home/rogerbernat/Documents/Projet_statistique_2024")
donnees <- readRDS("donnees_etape_0.Rdata", "rb")[,c(2,3,4,5)]
donnes_na <- na.omit(donnees)
scaled_data <- scale(donnees)
scaled_data_na <- na.omit(scaled_data)
library(RColorBrewer)
library(FactoMineR)
library(table1)
library(reshape)
library(umap)

```
## ACP
On effectue une ACP :

```{r}
donnees.PCA <- PCA(scaled_data_na, graph=FALSE)
plot(donnees.PCA, choix="var")
```

Observation des corrélations :

* Les éternuements et la toux pendant l'engraissement sont fortement corrélés entre eux.
* Les variables liées au post-sevrage sont également fortement corrélées entre elles, mais indépendantes des variables d'engraissement.


```{r}
plot(donnees.PCA$ind$coord)
```

En gardant les deux premiers axes, on obtient une variance expliquée supérieure à 60 %. On observe une distribution triangulaire des individus. On s'intéresse à la contribution des individus : 
```{r}
plot(donnees.PCA$ind$contrib)

top_contrib.1 <- order(donnees.PCA$ind$contrib[, 1], decreasing = TRUE)[1:7]
top_contrib.2 <- order(donnees.PCA$ind$contrib[, 2], decreasing = TRUE)[1:3]
top_contrib <- unique(c(top_contrib.1, top_contrib.2))
top_contrib

```

Observons que lors de la création des axes, huit individus ont principalement contribué à la création. Nous les excluons ensuite et effectuons à nouveau une ACP pour vérifier si cette répartition en forme de triangle est due à leur présence.
```{r}
donnees.PCA2 <- PCA(scaled_data_na, ind.sup = top_contrib)
plot(donnees.PCA2, choix="var")

```

On peut faire les mêmes observations que dans l'ACP précédente concernant la variance expliquée et la corrélation des variables. Examinons maintenant la distribution des individus selon ces axes :
```{r}
plot(rbind(donnees.PCA2$ind$coord, donnees.PCA2$ind.sup$coord))
```

On fait la même remarque que tout à l'heure. Regardons les contributions :
```{r}
plot(donnees.PCA2$ind$contrib)
```

Cependant, si on s'intéresse à la classification des individus sur les deux premiers axes, on arrive à distinguer que les individus semblent se diviser en 3 groupes :
```{r}
class.hierarchique1 <- hclust(dist(rbind(donnees.PCA2$ind$coord[,c(1, 2)], donnees.PCA2$ind.sup$coord[,c(1, 2)]), method = "euclidean"), method="ward.D2")
clusters1 <- cutree(class.hierarchique1, k = 3)
plot(class.hierarchique1)
```
```{r}
pca_data <- as.data.frame(rbind(donnees.PCA2$ind$coord[,c(1, 2)], donnees.PCA2$ind.sup$coord[,c(1, 2)]))
pca_data$Cluster <- as.factor(clusters1)

plot(pca_data$Dim.1, pca_data$Dim.2, col = pca_data$Cluster, pch = 19, 
     xlab = "PCA Dimension 1", ylab = "PCA Dimension 2", 
     main = "PCA Plot Colored by Clusters")
```

```{r, warning=FALSE}
meltData <- melt(scaled_data_na)
colors <- brewer.pal(3, "Set2")

boxplot(value ~ rep(clusters1, each = 4) * X2, data = meltData,
        horizontal =  TRUE,           
        outline = FALSE,  
        col = colors,  
        frame = FALSE,
        at = c(1,2,3,5,6,7,9,10,11,13,14,15),
        main = "Répartition des variables à expliquée par cluster acp",
        xlab = "Valeur",
        ylab = "",
        las = 2,
        cex.axis = 0.72,
        names=c("", "PS_TX", " ", " ", "PS_ETER", " ", " ", "ENG_TX", " ", " ", "ENG_ETER", " " )
        )

```

On observe le même phénomène que pour la première ACP, un petit group d'individus contribue fortement à la création des axes tandis que la contribution du reste des indivus reste relativement faible. De plus, les classes ne sont pas interprétables.
/*Caractérisation des clusters de la CAH après ACP ou ACM avec FactoMineR*/
```{r, echo = T, warning = F, message = F}
plot.HCPC(hc.acm1, choice = 'bar')
hc.acm1$desc.var
```
#obtenir sous forme de tableau les effectifs par cluster
```{r, echo = T, warning = F, message = F}
tab <- as.data.frame(freq(hc.acm1$data.clust$clust))
tab
```
#Autre option plus universelle pour caractériser la variable « cluster » par les variables qui ont permis de former les clusters ; en d’autres termes, l’objectif : identifier les variables qui caractérisent chaque cluster  suppose d’avoir récupéré l’affectation de chaque élevage à un cluster et d’avoir ajouté cette variable dans le fichier qui sera soumis à l’analyse décrite ci-dessous.
```{r, echo = T, warning = F, message = F}
print(catdes(datasetY_clust, num.var = 33, proba = 0.05)$category)   # NB : num.var = colonne où est la variable ‘cluster’ pour avoir les variables sur et sous représentées dans chaque cluster
```

## UMAP 

Ensuite on se propose d'utiliser une autre méthode de réduction des dimensions afin de comparer les résultats, c'est le cas de umap, qui est plus adaptée pour des variables avec des comportements non linéaires:

```{r}
library(umap)
donnees.umap <- umap(scaled_data_na, n_components = 2)
plot(donnees.umap$layout)
```

Cette fois-ci, on n'observe plus la répartition triangulaire. Passons à l'étape de classification hiérarchique :
```{r}
class.hierarchique <- hclust(dist(donnees.umap$layout, method = "euclidean"), method="ward.D2")
clusters <- cutree(class.hierarchique, k = 3)

plot(class.hierarchique)

```

Au vu du dendogramme, on décide de faire une classification en trois clusters qui se distribuent dans la plan ainsi :
```{r}
clusters <- cutree(class.hierarchique, k = 3)
plot(donnees.umap$layout, col = colors[clusters], pch = 19,
     main = "UMAP Layout with 3 Clusters",
     xlab = "UMAP 1", ylab = "UMAP 2")
```

Passons à l'interprétation des clusters:

```{r}
clusters <- as.factor(clusters)

boxplot(value ~ rep(clusters, 4) * X2, data = meltData,
        horizontal =  TRUE,           
        outline = FALSE,  
        col = colors,  
        frame = FALSE,
        at = c(1,2,3,5,6,7,9,10,11,13,14,15),
        main = "Répartition des variables à expliquer normalisées par cluster umap",
        xlab = "Valeur",
        ylab = "",
        las = 2,
        cex.axis = 0.72,
        names=c("", "PS_TX", " ", " ", "PS_ETER", " ", " ", "ENG_TX", " ", " ", "ENG_ETER", " " )
        )
```

* Cluster orange : Contient majoritairement les individus malades en post-sevrage.
* Cluster vert : Contient les individus malades en engraissement, bien que cet effet soit moins marqué.
* Cluster bleu : Représente les individus en bonne santé.
